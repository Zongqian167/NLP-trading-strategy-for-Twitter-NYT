{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install webdriver-manager\n",
    "# from selenium import webdriver\n",
    "# from webdriver_manager.chrome import ChromeDriverManager\n",
    "# driver = webdriver.Chrome(ChromeDriverManager().install())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # read more\n",
    "# for i in range(12):\n",
    "#     browser.find_element_by_xpath('//*[@id=\"site-content\"]/div/div[2]/div[3]/div/button').click()\n",
    "#     time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.set_option('max_colwidth',1000)\n",
    "# df[['date','link']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Crawler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Open webdriver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "import time\n",
    "from selenium import webdriver\n",
    "import random\n",
    "import re\n",
    "import datetime\n",
    "\n",
    "chromedir = \"C:\\Program Files (x86)\\Google\\Chrome\\Application\"\n",
    "os.chdir(chromedir)\n",
    "browser = webdriver.Chrome()\n",
    "browser.get('https://myaccount.nytimes.com/auth/login?response_type=cookie&client_id=vi')\n",
    "time.sleep(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Log in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: please type American Airlines into search box\n"
     ]
    }
   ],
   "source": [
    "username = 'bj2438@columbia.edu'\n",
    "browser.find_element_by_id('email').clear()\n",
    "browser.find_element_by_id('email').send_keys(username)\n",
    "time.sleep(0.5 + 3*random.random())\n",
    "browser.find_element_by_xpath('''//*[@id=\"myAccountAuth\"]/div[1]/div/div/form/div/div[2]/button''').click()\n",
    "time.sleep(0.5 + 3*random.random())\n",
    "password = '19980526@Columbia'\n",
    "browser.find_element_by_id('password').clear()\n",
    "browser.find_element_by_id('password').send_keys(password)\n",
    "time.sleep(1 + 2*random.random())\n",
    "browser.find_element_by_xpath('''//*[@id=\"myAccountAuth\"]/div[1]/div/form/div/div[2]/button''').click()\n",
    "time.sleep(5 + 2*random.random())\n",
    "print('Note: please type American Airlines into search box')\n",
    "# ------------------------------------login finished---------------------------------------------\n",
    "# -----------------------手动搜索----American Airlines-------PepsiCo--------Disney----------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare urls & Save cookies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "date range set from 01/01/2010 to 06/30/2021, done\n"
     ]
    }
   ],
   "source": [
    "# specify date range\n",
    "a=120 # read more times\n",
    "browser.find_element_by_class_name('css-p5555t').click()\n",
    "time.sleep(0.5)\n",
    "browser.find_element_by_xpath('//*[@id=\"site-content\"]/div/div[1]/div[2]/div/div/div[1]/div/div/div/ul/li[6]/button').click()\n",
    "time.sleep(0.5)\n",
    "start = '01/01/2010'\n",
    "browser.find_element_by_id('startDate').clear()\n",
    "browser.find_element_by_id('startDate').send_keys(start)\n",
    "end = '06/30/2021'\n",
    "browser.find_element_by_id('endDate').clear()\n",
    "browser.find_element_by_id('endDate').send_keys(end)\n",
    "print(f'date range set from {start} to {end}, done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "search order set by relevance, done\n"
     ]
    }
   ],
   "source": [
    "# order by newest\n",
    "browser.find_element_by_class_name('css-v7it2b').click()\n",
    "browser.find_element_by_xpath('//*[@id=\"site-content\"]/div/div[1]/div[1]/form/div[2]/div/select/option[1]').click()\n",
    "print('search order set by relevance, done')\n",
    "# browser.find_element_by_xpath('//*[@id=\"site-content\"]/div/div[1]/div[1]/form/div[2]/div/select/option[2]').click()\n",
    "# print('search order set from newest to oldest, done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start crawling...\n",
      "load 10 / 120 times...\n",
      "load 20 / 120 times...\n",
      "load 30 / 120 times...\n",
      "load 40 / 120 times...\n",
      "load 50 / 120 times...\n",
      "load 60 / 120 times...\n",
      "load 70 / 120 times...\n",
      "load 80 / 120 times...\n",
      "load 90 / 120 times...\n",
      "load 100 / 120 times...\n",
      "load 110 / 120 times...\n",
      "load 120 / 120 times...\n",
      "finish loading!\n",
      "basic information gathered\n",
      "actual/maximum articles in the end:  1010/1210\n",
      "dataframe built\n",
      "cookies done\n"
     ]
    }
   ],
   "source": [
    "# specify date range\n",
    "a=120 # read more times\n",
    "print('start crawling...')\n",
    "\n",
    "# ------------------------------------ready for crawling------------------------------------------\n",
    "\n",
    "# Important!\n",
    "ct = 0\n",
    "try:\n",
    "    for i in range(a):\n",
    "        browser.find_element_by_xpath('//*[@id=\"site-content\"]/div/div[2]/div[3]/div/button').click()\n",
    "        ct += 1\n",
    "        if ct%10 == 0:\n",
    "            print(f'load {ct} / {a} times...')\n",
    "        time.sleep(2)\n",
    "except:\n",
    "    print('All articles loaded, or ERRORs happened!')\n",
    "print('finish loading!')\n",
    "    \n",
    "\n",
    "results_page = BeautifulSoup(browser.page_source,'lxml')\n",
    "\n",
    "dp = results_page.find_all('div',class_='css-e1lvw9')\n",
    "date_ls = results_page.find_all('span',class_='css-17ubb9w')\n",
    "n = len(dp)\n",
    "date = []\n",
    "title = []\n",
    "abstract = []\n",
    "author = []\n",
    "link = []\n",
    "genre = []\n",
    "\n",
    "for i in range(n):\n",
    "#     try:\n",
    "#         date.append(re.search(r'/(\\d{4}/\\d{2}/\\d{2})', dp[i].find('a').get('href')).groups()[0])\n",
    "#     except:\n",
    "#         date.append(date_ls[i].get_text())\n",
    "    original_date = date_ls[i].get_text()\n",
    "    if ',' not in original_date:\n",
    "        original_date += ', 2021'\n",
    "    new_date = original_date.replace('Jan.','01,').replace('Feb.','02,').replace('March','03,').replace('April','04,').replace('May','05,').replace('June','06,').replace('July','07,').replace('Aug.','08,').replace('Sept.','09,').replace('Oct.','10,').replace('Nov.','11,').replace('Dec.','12,').replace(', ','/')\n",
    "    date.append(datetime.datetime.strftime(datetime.datetime.strptime(new_date,'%m/%d/%Y'),'%Y/%m/%d'))\n",
    "    title.append(dp[i].find('h4',class_='css-2fgx4k').get_text())\n",
    "    try:\n",
    "        abstract.append(dp[i].find('p',class_='css-16nhkrn').get_text())\n",
    "    except:\n",
    "        abstract.append('')\n",
    "    try:\n",
    "        author.append(dp[i].find('p',class_='css-15w69y9').get_text()[3:])\n",
    "    except:\n",
    "        author.append('')\n",
    "    l = dp[i].find('a').get('href')\n",
    "    try:\n",
    "        if l[:6] == 'https:':\n",
    "            link.append(l)\n",
    "        else:\n",
    "            link.append('https://www.nytimes.com' + l)\n",
    "    except:\n",
    "        link.append('')\n",
    "    genre.append(dp[i].find('p',class_='css-myxawk').get_text())\n",
    "print('basic information gathered')\n",
    "print(f'actual/maximum articles in the end:  {n}/{a*10+10}')\n",
    "    \n",
    "# -----------------------------------------finish crawling------------------------------------------\n",
    "# Note: There maybe a few missing values for abstract and author, \n",
    "#       and date format mostly be like YYYY/MM/DD, few like Sept. 11, 2019 or June 11\n",
    "\n",
    "df = pd.DataFrame()\n",
    "df['date'] = np.array(date)\n",
    "df['title'] = np.array(title)\n",
    "df['abstract'] = np.array(abstract)\n",
    "df['author'] = np.array(author)\n",
    "df['link'] = np.array(link)\n",
    "df['genre'] = np.array(genre)\n",
    "df['article'] = ''\n",
    "df\n",
    "print('dataframe built')\n",
    "\n",
    "c = browser.get_cookies()\n",
    "cookies = {}\n",
    "for cookie in c:\n",
    "    cookies[cookie['name']] = cookie['value']\n",
    "print('cookies done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First trial of getting articles content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete null link records\n",
    "df = df[df['link']!='']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gather single article content...\n",
      "articles 50/1009\n",
      "articles 100/1009\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "102",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mget_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   2894\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2895\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2896\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.Int64HashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.Int64HashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 102",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-48-ea7aba413e10>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m     \u001b[1;31m# 使用该cookie完成请求\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m     \u001b[0mu\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'link'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m     \u001b[0mresponse\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrequests\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mu\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcookies\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcookies\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m     \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0.2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m    877\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    878\u001b[0m             \u001b[0mmaybe_callable\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_if_callable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 879\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getitem_axis\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmaybe_callable\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    880\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    881\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_is_scalar_access\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTuple\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py\u001b[0m in \u001b[0;36m_getitem_axis\u001b[1;34m(self, key, axis)\u001b[0m\n\u001b[0;32m   1108\u001b[0m         \u001b[1;31m# fall thru to straight lookup\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1109\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_validate_key\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1110\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_label\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1111\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1112\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_get_slice_axis\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mslice_obj\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mslice\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py\u001b[0m in \u001b[0;36m_get_label\u001b[1;34m(self, label, axis)\u001b[0m\n\u001b[0;32m   1057\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_get_label\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1058\u001b[0m         \u001b[1;31m# GH#5667 this will fail if the label is not present in the axis.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1059\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mxs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1060\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1061\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_handle_lowerdim_multi_index_axis0\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtup\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTuple\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36mxs\u001b[1;34m(self, key, axis, level, drop_level)\u001b[0m\n\u001b[0;32m   3489\u001b[0m             \u001b[0mloc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnew_index\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc_level\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdrop_level\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdrop_level\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3490\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3491\u001b[1;33m             \u001b[0mloc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3492\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3493\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mget_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   2895\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2896\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2897\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2898\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2899\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mtolerance\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 102"
     ]
    }
   ],
   "source": [
    "headers = {\n",
    "    'method': 'GET',\n",
    "    'path': '/',\n",
    "    'scheme': 'https',\n",
    "    'accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3',\n",
    "    'accept-encoding': 'gzip, deflate, br',\n",
    "    'accept-language': 'zh-CN,zh;q=0.9',\n",
    "    'cache-control': 'max-age=0',\n",
    "    'upgrade-insecure-requests': '1',\n",
    "    'user-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/74.0.3729.169 Safari/537.36',\n",
    "\n",
    "}\n",
    "\n",
    "print('gather single article content...')\n",
    "\n",
    "num_of_success = 0\n",
    "\n",
    "for k in range(len(df)):\n",
    "    # 使用该cookie完成请求\n",
    "    u = df['link'].loc[k]\n",
    "    response = requests.get(url=u, headers=headers, cookies=cookies)\n",
    "    time.sleep(0.2)\n",
    "    results_page = BeautifulSoup(response.content,'lxml')\n",
    "\n",
    "    if u[24:29] == 'video' or u[24:33] == 'slideshow' or u[24:31] == 'podcast':\n",
    "        s = 'Not Applicable'\n",
    "    \n",
    "    elif u[:16] == 'https://www.nyti':\n",
    "        paras = results_page.find_all('div',class_='css-53u6y8')\n",
    "        # paras = paras[:-1]\n",
    "\n",
    "        # 存在无内容的段落，按''处理\n",
    "        # 有内容的段落直接连接在一起\n",
    "        s = ''\n",
    "        for j in range(len(paras)):\n",
    "            ls = paras[j].find_all('p')\n",
    "            try:\n",
    "                s += paras[j].find('h2', class_ = 'css-1aoo5yy').get_text()\n",
    "            except:\n",
    "                pass\n",
    "            s += ' '\n",
    "            for i in range(len(ls)):\n",
    "                s += ls[i].get_text()\n",
    "                s += ' '\n",
    "\n",
    "#     elif u[:16] == 'https://intransi' or u[:16] == 'https://dealbook' or u[:16] == 'https://bucks.bl':\n",
    "    else:\n",
    "        paras = results_page.find_all('p',class_='story-body-text')\n",
    "        s = ''\n",
    "        for j in range(len(paras)):\n",
    "            s += paras[j].get_text().replace('  ', '').replace('\\n',' ').replace('\\xa0','')\n",
    "            \n",
    "    df['article'].loc[k] = s.strip()\n",
    "    \n",
    "    if df['article'].loc[k] != '':\n",
    "        num_of_success += 1\n",
    "\n",
    "    if k % 50 == 49 or k+1 == len(df):\n",
    "        print(f'articles {k+1}/{len(df)}')\n",
    "    time.sleep(0.2)\n",
    "\n",
    "print(f'finished the initial trial of getting articles, with {num_of_success}/{n} of success.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Drop duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unique/total: 761/1009\n",
      "duplicates dropped!\n",
      "unique/total: 761/761\n"
     ]
    }
   ],
   "source": [
    "df['magic'] = df['date']+df['title']\n",
    "print(f\"unique/total: {len(df['magic'].unique())}/{len(df['magic'])}\")\n",
    "df_new = df.drop_duplicates(['magic']).reset_index()\n",
    "print('duplicates dropped!')\n",
    "print(f\"unique/total: {len(df_new['magic'].unique())}/{len(df_new['magic'])}\")\n",
    "del df_new['magic']## Check the missing articles\n",
    "df_stored, df = df, df_new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check the missing articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cookies exists\n",
      "check the missing articles 10 times...\n",
      "start!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\woody\\anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py:670: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  iloc._setitem_with_indexer(indexer, value)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checking round 1:  articles 57/761: success!\n",
      "checking round 1:  articles 71/761: success!\n",
      "checking round 1:  articles 76/761: success!\n",
      "checking round 1:  articles 98/761: success!\n",
      "checking round 1:  articles 99/761: success!\n",
      "checking round 1:  articles 100/761: success!\n",
      "checking round 1:  articles 101/761: fail...\n",
      "checking round 1:  articles 102/761: success!\n",
      "checking round 1:  articles 103/761: success!\n",
      "checking round 1:  articles 104/761: success!\n",
      "checking round 1:  articles 105/761: success!\n",
      "checking round 1:  articles 106/761: success!\n",
      "checking round 1:  articles 107/761: success!\n",
      "checking round 1:  articles 108/761: success!\n",
      "checking round 1:  articles 109/761: success!\n",
      "checking round 1:  articles 110/761: fail...\n",
      "checking round 1:  articles 111/761: success!\n",
      "checking round 1:  articles 112/761: success!\n",
      "checking round 1:  articles 113/761: success!\n",
      "checking round 1:  articles 114/761: fail...\n",
      "checking round 1:  articles 115/761: success!\n",
      "checking round 1:  articles 116/761: success!\n",
      "checking round 1:  articles 117/761: success!\n",
      "checking round 1:  articles 118/761: success!\n",
      "checking round 1:  articles 119/761: success!\n",
      "checking round 1:  articles 120/761: success!\n",
      "checking round 1:  articles 121/761: success!\n",
      "checking round 1:  articles 122/761: success!\n",
      "checking round 1:  articles 123/761: success!\n",
      "checking round 1:  articles 124/761: success!\n",
      "checking round 1:  articles 125/761: success!\n",
      "checking round 1:  articles 126/761: success!\n",
      "checking round 1:  articles 127/761: success!\n",
      "checking round 1:  articles 128/761: success!\n",
      "checking round 1:  articles 129/761: success!\n",
      "checking round 1:  articles 130/761: success!\n",
      "checking round 1:  articles 131/761: success!\n",
      "checking round 1:  articles 132/761: success!\n",
      "checking round 1:  articles 133/761: success!\n",
      "checking round 1:  articles 134/761: success!\n",
      "checking round 1:  articles 135/761: success!\n",
      "checking round 1:  articles 136/761: success!\n",
      "checking round 1:  articles 137/761: success!\n",
      "checking round 1:  articles 138/761: success!\n",
      "checking round 1:  articles 139/761: success!\n",
      "checking round 1:  articles 140/761: success!\n",
      "checking round 1:  articles 141/761: success!\n",
      "checking round 1:  articles 142/761: success!\n",
      "checking round 1:  articles 143/761: success!\n",
      "checking round 1:  articles 144/761: success!\n",
      "checking round 1:  articles 145/761: success!\n",
      "checking round 1:  articles 146/761: success!\n",
      "checking round 1:  articles 147/761: success!\n",
      "checking round 1:  articles 148/761: success!\n",
      "checking round 1:  articles 149/761: success!\n",
      "checking round 1:  articles 150/761: success!\n",
      "checking round 1:  articles 151/761: success!\n",
      "checking round 1:  articles 152/761: success!\n",
      "checking round 1:  articles 153/761: success!\n",
      "checking round 1:  articles 154/761: success!\n",
      "checking round 1:  articles 155/761: success!\n",
      "checking round 1:  articles 156/761: success!\n",
      "checking round 1:  articles 157/761: fail...\n",
      "checking round 1:  articles 158/761: success!\n",
      "checking round 1:  articles 159/761: success!\n",
      "checking round 1:  articles 160/761: success!\n",
      "checking round 1:  articles 161/761: success!\n",
      "checking round 1:  articles 162/761: success!\n",
      "checking round 1:  articles 163/761: success!\n",
      "checking round 1:  articles 164/761: success!\n",
      "checking round 1:  articles 165/761: success!\n",
      "checking round 1:  articles 166/761: success!\n",
      "checking round 1:  articles 167/761: success!\n",
      "checking round 1:  articles 168/761: fail...\n",
      "checking round 1:  articles 169/761: success!\n",
      "checking round 1:  articles 170/761: success!\n",
      "checking round 1:  articles 171/761: success!\n",
      "checking round 1:  articles 172/761: success!\n",
      "checking round 1:  articles 173/761: success!\n",
      "checking round 1:  articles 174/761: success!\n",
      "checking round 1:  articles 175/761: success!\n",
      "checking round 1:  articles 176/761: success!\n",
      "checking round 1:  articles 177/761: success!\n",
      "checking round 1:  articles 178/761: success!\n",
      "checking round 1:  articles 179/761: success!\n",
      "checking round 1:  articles 180/761: success!\n",
      "checking round 1:  articles 181/761: success!\n",
      "checking round 1:  articles 182/761: success!\n",
      "checking round 1:  articles 183/761: success!\n",
      "checking round 1:  articles 184/761: success!\n",
      "checking round 1:  articles 185/761: success!\n",
      "checking round 1:  articles 186/761: success!\n",
      "checking round 1:  articles 187/761: fail...\n",
      "checking round 1:  articles 188/761: success!\n",
      "checking round 1:  articles 189/761: success!\n",
      "checking round 1:  articles 190/761: success!\n",
      "checking round 1:  articles 191/761: success!\n",
      "checking round 1:  articles 192/761: success!\n",
      "checking round 1:  articles 193/761: success!\n",
      "checking round 1:  articles 194/761: success!\n",
      "checking round 1:  articles 195/761: success!\n",
      "checking round 1:  articles 196/761: success!\n",
      "checking round 1:  articles 197/761: success!\n",
      "checking round 1:  articles 198/761: success!\n",
      "checking round 1:  articles 199/761: success!\n",
      "checking round 1:  articles 200/761: success!\n",
      "checking round 1:  articles 201/761: success!\n",
      "checking round 1:  articles 202/761: success!\n",
      "checking round 1:  articles 203/761: success!\n",
      "checking round 1:  articles 204/761: success!\n",
      "checking round 1:  articles 205/761: success!\n",
      "checking round 1:  articles 206/761: success!\n",
      "checking round 1:  articles 207/761: success!\n",
      "checking round 1:  articles 208/761: success!\n",
      "checking round 1:  articles 209/761: success!\n",
      "checking round 1:  articles 210/761: success!\n",
      "checking round 1:  articles 211/761: success!\n",
      "checking round 1:  articles 212/761: success!\n",
      "checking round 1:  articles 213/761: success!\n",
      "checking round 1:  articles 214/761: success!\n",
      "checking round 1:  articles 215/761: success!\n",
      "checking round 1:  articles 216/761: success!\n",
      "checking round 1:  articles 217/761: success!\n",
      "checking round 1:  articles 218/761: success!\n",
      "checking round 1:  articles 219/761: success!\n",
      "checking round 1:  articles 220/761: success!\n",
      "checking round 1:  articles 221/761: success!\n",
      "checking round 1:  articles 222/761: success!\n",
      "checking round 1:  articles 223/761: success!\n",
      "checking round 1:  articles 224/761: success!\n",
      "checking round 1:  articles 225/761: success!\n",
      "checking round 1:  articles 226/761: success!\n",
      "checking round 1:  articles 227/761: success!\n",
      "checking round 1:  articles 228/761: success!\n",
      "checking round 1:  articles 229/761: success!\n",
      "checking round 1:  articles 230/761: success!\n",
      "checking round 1:  articles 231/761: success!\n",
      "checking round 1:  articles 232/761: success!\n",
      "checking round 1:  articles 233/761: success!\n",
      "checking round 1:  articles 234/761: success!\n",
      "checking round 1:  articles 235/761: success!\n",
      "checking round 1:  articles 236/761: success!\n",
      "checking round 1:  articles 237/761: success!\n",
      "checking round 1:  articles 238/761: success!\n",
      "checking round 1:  articles 239/761: success!\n",
      "checking round 1:  articles 240/761: success!\n",
      "checking round 1:  articles 241/761: success!\n",
      "checking round 1:  articles 242/761: success!\n",
      "checking round 1:  articles 243/761: success!\n",
      "checking round 1:  articles 244/761: success!\n",
      "checking round 1:  articles 245/761: success!\n",
      "checking round 1:  articles 246/761: fail...\n",
      "checking round 1:  articles 247/761: success!\n",
      "checking round 1:  articles 248/761: success!\n",
      "checking round 1:  articles 249/761: success!\n",
      "checking round 1:  articles 250/761: success!\n",
      "checking round 1:  articles 251/761: success!\n",
      "checking round 1:  articles 252/761: success!\n",
      "checking round 1:  articles 253/761: success!\n",
      "checking round 1:  articles 254/761: success!\n",
      "checking round 1:  articles 255/761: success!\n",
      "checking round 1:  articles 256/761: success!\n",
      "checking round 1:  articles 257/761: fail...\n",
      "checking round 1:  articles 258/761: success!\n",
      "checking round 1:  articles 259/761: success!\n",
      "checking round 1:  articles 260/761: success!\n"
     ]
    },
    {
     "ename": "ConnectionError",
     "evalue": "HTTPSConnectionPool(host='6thfloor.blogs.nytimes.com', port=443): Max retries exceeded with url: /2014/03/11/talking-with-readers-about-owen-disney-and-autism/?searchResultPosition=327 (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x0000018809BBAD30>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mgaierror\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\urllib3\\connection.py\u001b[0m in \u001b[0;36m_new_conn\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    158\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 159\u001b[1;33m             conn = connection.create_connection(\n\u001b[0m\u001b[0;32m    160\u001b[0m                 \u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dns_host\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mport\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mextra_kw\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\urllib3\\util\\connection.py\u001b[0m in \u001b[0;36mcreate_connection\u001b[1;34m(address, timeout, source_address, socket_options)\u001b[0m\n\u001b[0;32m     60\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 61\u001b[1;33m     \u001b[1;32mfor\u001b[0m \u001b[0mres\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msocket\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetaddrinfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhost\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mport\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfamily\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msocket\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSOCK_STREAM\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     62\u001b[0m         \u001b[0maf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msocktype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mproto\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcanonname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msa\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mres\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\socket.py\u001b[0m in \u001b[0;36mgetaddrinfo\u001b[1;34m(host, port, family, type, proto, flags)\u001b[0m\n\u001b[0;32m    917\u001b[0m     \u001b[0maddrlist\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 918\u001b[1;33m     \u001b[1;32mfor\u001b[0m \u001b[0mres\u001b[0m \u001b[1;32min\u001b[0m \u001b[0m_socket\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetaddrinfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhost\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mport\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfamily\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mproto\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    919\u001b[0m         \u001b[0maf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msocktype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mproto\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcanonname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msa\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mres\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mgaierror\u001b[0m: [Errno 11001] getaddrinfo failed",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mNewConnectionError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\urllib3\\connectionpool.py\u001b[0m in \u001b[0;36murlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[0;32m    669\u001b[0m             \u001b[1;31m# Make the request on the httplib connection object.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 670\u001b[1;33m             httplib_response = self._make_request(\n\u001b[0m\u001b[0;32m    671\u001b[0m                 \u001b[0mconn\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\urllib3\\connectionpool.py\u001b[0m in \u001b[0;36m_make_request\u001b[1;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[0;32m    380\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 381\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_validate_conn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    382\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mSocketTimeout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mBaseSSLError\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\urllib3\\connectionpool.py\u001b[0m in \u001b[0;36m_validate_conn\u001b[1;34m(self, conn)\u001b[0m\n\u001b[0;32m    977\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"sock\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# AppEngine might not have  `.sock`\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 978\u001b[1;33m             \u001b[0mconn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconnect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    979\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\urllib3\\connection.py\u001b[0m in \u001b[0;36mconnect\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    308\u001b[0m         \u001b[1;31m# Add certificate verification\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 309\u001b[1;33m         \u001b[0mconn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_new_conn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    310\u001b[0m         \u001b[0mhostname\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhost\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\urllib3\\connection.py\u001b[0m in \u001b[0;36m_new_conn\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    170\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mSocketError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 171\u001b[1;33m             raise NewConnectionError(\n\u001b[0m\u001b[0;32m    172\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"Failed to establish a new connection: %s\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNewConnectionError\u001b[0m: <urllib3.connection.HTTPSConnection object at 0x0000018809BBAD30>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mMaxRetryError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\requests\\adapters.py\u001b[0m in \u001b[0;36msend\u001b[1;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[0;32m    438\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mchunked\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 439\u001b[1;33m                 resp = conn.urlopen(\n\u001b[0m\u001b[0;32m    440\u001b[0m                     \u001b[0mmethod\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\urllib3\\connectionpool.py\u001b[0m in \u001b[0;36murlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[0;32m    725\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 726\u001b[1;33m             retries = retries.increment(\n\u001b[0m\u001b[0;32m    727\u001b[0m                 \u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merror\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_pool\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_stacktrace\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexc_info\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\urllib3\\util\\retry.py\u001b[0m in \u001b[0;36mincrement\u001b[1;34m(self, method, url, response, error, _pool, _stacktrace)\u001b[0m\n\u001b[0;32m    445\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mnew_retry\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_exhausted\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 446\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mMaxRetryError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_pool\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merror\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mResponseError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcause\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    447\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mMaxRetryError\u001b[0m: HTTPSConnectionPool(host='6thfloor.blogs.nytimes.com', port=443): Max retries exceeded with url: /2014/03/11/talking-with-readers-about-owen-disney-and-autism/?searchResultPosition=327 (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x0000018809BBAD30>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mConnectionError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-51-d4877015321f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     18\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'article'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m''\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m             \u001b[0mu\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'link'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m             \u001b[0mresponse\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrequests\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mu\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcookies\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcookies\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m             \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0.2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m             \u001b[0mresults_page\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mBeautifulSoup\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontent\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'lxml'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\requests\\api.py\u001b[0m in \u001b[0;36mget\u001b[1;34m(url, params, **kwargs)\u001b[0m\n\u001b[0;32m     74\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     75\u001b[0m     \u001b[0mkwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msetdefault\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'allow_redirects'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 76\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mrequest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'get'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     77\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     78\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\requests\\api.py\u001b[0m in \u001b[0;36mrequest\u001b[1;34m(method, url, **kwargs)\u001b[0m\n\u001b[0;32m     59\u001b[0m     \u001b[1;31m# cases, and look like a memory leak in others.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     60\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0msessions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0msession\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 61\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0msession\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     62\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\requests\\sessions.py\u001b[0m in \u001b[0;36mrequest\u001b[1;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[0;32m    528\u001b[0m         }\n\u001b[0;32m    529\u001b[0m         \u001b[0msend_kwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msettings\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 530\u001b[1;33m         \u001b[0mresp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprep\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0msend_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    531\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    532\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mresp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\requests\\sessions.py\u001b[0m in \u001b[0;36msend\u001b[1;34m(self, request, **kwargs)\u001b[0m\n\u001b[0;32m    641\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    642\u001b[0m         \u001b[1;31m# Send the request\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 643\u001b[1;33m         \u001b[0mr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0madapter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    644\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    645\u001b[0m         \u001b[1;31m# Total elapsed time of the request (approximately)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\requests\\adapters.py\u001b[0m in \u001b[0;36msend\u001b[1;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[0;32m    514\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0mSSLError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    515\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 516\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mConnectionError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    517\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    518\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mClosedPoolError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mConnectionError\u001b[0m: HTTPSConnectionPool(host='6thfloor.blogs.nytimes.com', port=443): Max retries exceeded with url: /2014/03/11/talking-with-readers-about-owen-disney-and-autism/?searchResultPosition=327 (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x0000018809BBAD30>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))"
     ]
    }
   ],
   "source": [
    "if not c:\n",
    "    c = browser.get_cookies()\n",
    "    cookies = {}\n",
    "    for cookie in c:\n",
    "        cookies[cookie['name']] = cookie['value']\n",
    "    print('cookies done')\n",
    "else:\n",
    "    print('cookies exists')\n",
    "\n",
    "    \n",
    "total_check_times = 10\n",
    "    \n",
    "print(f'check the missing articles {total_check_times} times...')\n",
    "print('start!')\n",
    "\n",
    "for c in range(total_check_times):\n",
    "    for k in range(len(df)):\n",
    "        if df['article'].loc[k] == '':\n",
    "            u = df['link'].loc[k]\n",
    "            response = requests.get(url=u, headers=headers, cookies=cookies)\n",
    "            time.sleep(0.2)\n",
    "            results_page = BeautifulSoup(response.content,'lxml')\n",
    "            \n",
    "            if u[24:29] == 'video' or u[24:33] == 'slideshow' or u[24:31] == 'podcast':\n",
    "                s = 'Not Applicable'\n",
    "                \n",
    "            elif u[:16] == 'https://www.nyti':\n",
    "                paras = results_page.find_all('div',class_='css-53u6y8')\n",
    "                # paras = paras[:-1]\n",
    "\n",
    "                # 存在无内容的段落，按''处理\n",
    "                # 有内容的段落直接连接在一起\n",
    "                s = ''\n",
    "                for j in range(len(paras)):\n",
    "                    ls = paras[j].find_all('p')\n",
    "                    try:\n",
    "                        s += paras[j].find('h2', class_ = 'css-1aoo5yy').get_text()\n",
    "                    except:\n",
    "                        pass\n",
    "                    s += ' '\n",
    "                    for i in range(len(ls)):\n",
    "                        s += ls[i].get_text()\n",
    "                        s += ' '\n",
    "\n",
    "        #     elif u[:16] == 'https://intransi' or u[:16] == 'https://dealbook' or u[:16] == 'https://bucks.bl':\n",
    "            else:\n",
    "                paras = results_page.find_all('p',class_='story-body-text')\n",
    "                s = ''\n",
    "                for j in range(len(paras)):\n",
    "                    s += paras[j].get_text().replace('  ', '').replace('\\n',' ').replace('\\xa0','')\n",
    "\n",
    "            df['article'].loc[k] = s.strip()\n",
    "            if df['article'].loc[k] != '':\n",
    "                state = 'success!'\n",
    "            else:\n",
    "                state = 'fail...'\n",
    "            print(f'checking round {c+1}:  articles {k+1}/{len(df)}: {state}')\n",
    "            time.sleep(0.2)\n",
    "            \n",
    "    print(f\"Round {c+1}/{total_check_times} finished: missing/all articles {len(df[df['article']==''])}/{len(df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "u = 'https://intransit.blogs.nytimes.com/2010/03/10/travel-deals-airtran-and-american-airlines-sales/?searchResultPosition=39'\n",
    "response = requests.get(url=u, headers=headers, cookies=cookies)\n",
    "results_page = BeautifulSoup(response.content,'lxml')\n",
    "paras = results_page.find_all('p',class_='story-body-text')\n",
    "s = ''\n",
    "for j in range(len(paras)):\n",
    "    s += paras[j].get_text().replace('  ', '').replace('\\n',' ').replace('\\xa0','')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>title</th>\n",
       "      <th>abstract</th>\n",
       "      <th>author</th>\n",
       "      <th>genre</th>\n",
       "      <th>article</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2021/05/14</td>\n",
       "      <td>A Tahini Lemon Cranberry Bar Dreams of Whole F...</td>\n",
       "      <td>With workers at home and stores stingy with sh...</td>\n",
       "      <td>Julie Creswell</td>\n",
       "      <td>Business</td>\n",
       "      <td>Lupii bars were supposed to have a breakout ye...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2021/02/09</td>\n",
       "      <td>Aunt Jemima Has a New Name After 131 Years: Th...</td>\n",
       "      <td>Quaker Oats announced it would drop the name A...</td>\n",
       "      <td>Neil Vigdor</td>\n",
       "      <td>Business</td>\n",
       "      <td>It has been a staple of American breakfast tab...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2011/09/20</td>\n",
       "      <td>PepsiCo to Foster Chickpeas in Ethiopia</td>\n",
       "      <td>The company said an increased yield of the cro...</td>\n",
       "      <td>Stephanie Strom</td>\n",
       "      <td>Business</td>\n",
       "      <td>PepsiCo plans to announce a new venture on Wed...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2013/01/25</td>\n",
       "      <td>PepsiCo Will Halt Use of Additive in Gatorade</td>\n",
       "      <td>A petition on Change.org gets results.</td>\n",
       "      <td>Stephanie Strom</td>\n",
       "      <td>Style</td>\n",
       "      <td>PepsiCo announced on Friday that it would no l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2010/03/16</td>\n",
       "      <td>PepsiCo Adds to Buyback Plan</td>\n",
       "      <td>PepsiCo said Monday that it would buy back up ...</td>\n",
       "      <td>Dealbook</td>\n",
       "      <td>Business</td>\n",
       "      <td>PepsiCo said Monday that it would buy back up ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>734</th>\n",
       "      <td>2020/09/29</td>\n",
       "      <td>Corrections: Sept. 30, 2020</td>\n",
       "      <td>Corrections that appeared in print on Wednesda...</td>\n",
       "      <td></td>\n",
       "      <td>Corrections</td>\n",
       "      <td>NATIONAL An article on Monday about President ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>735</th>\n",
       "      <td>2014/01/06</td>\n",
       "      <td>F.D.A. Must Act on Food</td>\n",
       "      <td>Michael F. Jacobson of the Center for Science ...</td>\n",
       "      <td></td>\n",
       "      <td>Opinion</td>\n",
       "      <td>To the Editor:  “Social Media as a Megaphone t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>736</th>\n",
       "      <td>2016/10/17</td>\n",
       "      <td>Coke, Pepsi and Climate Change</td>\n",
       "      <td>A U.S. senator says the lobbying presence in C...</td>\n",
       "      <td></td>\n",
       "      <td>Opinion</td>\n",
       "      <td>To the Editor: Re “Soda Giants Back, Then Lobb...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>737</th>\n",
       "      <td>2017/08/04</td>\n",
       "      <td>Corrections: August 5, 2017</td>\n",
       "      <td>Corrections appearing in print on Saturday, Au...</td>\n",
       "      <td></td>\n",
       "      <td>Corrections</td>\n",
       "      <td>BUSINESS DAY An article on Thursday about a ne...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>738</th>\n",
       "      <td>2015/10/01</td>\n",
       "      <td>Ways to Combat Global Warming</td>\n",
       "      <td>A discussion of the need for political engagem...</td>\n",
       "      <td></td>\n",
       "      <td>Opinion</td>\n",
       "      <td>To the Editor: Re “Progress Seen on Warming, W...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>739 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           date                                              title  \\\n",
       "0    2021/05/14  A Tahini Lemon Cranberry Bar Dreams of Whole F...   \n",
       "1    2021/02/09  Aunt Jemima Has a New Name After 131 Years: Th...   \n",
       "2    2011/09/20            PepsiCo to Foster Chickpeas in Ethiopia   \n",
       "3    2013/01/25      PepsiCo Will Halt Use of Additive in Gatorade   \n",
       "4    2010/03/16                       PepsiCo Adds to Buyback Plan   \n",
       "..          ...                                                ...   \n",
       "734  2020/09/29                        Corrections: Sept. 30, 2020   \n",
       "735  2014/01/06                            F.D.A. Must Act on Food   \n",
       "736  2016/10/17                     Coke, Pepsi and Climate Change   \n",
       "737  2017/08/04                        Corrections: August 5, 2017   \n",
       "738  2015/10/01                      Ways to Combat Global Warming   \n",
       "\n",
       "                                              abstract           author  \\\n",
       "0    With workers at home and stores stingy with sh...   Julie Creswell   \n",
       "1    Quaker Oats announced it would drop the name A...      Neil Vigdor   \n",
       "2    The company said an increased yield of the cro...  Stephanie Strom   \n",
       "3               A petition on Change.org gets results.  Stephanie Strom   \n",
       "4    PepsiCo said Monday that it would buy back up ...         Dealbook   \n",
       "..                                                 ...              ...   \n",
       "734  Corrections that appeared in print on Wednesda...                    \n",
       "735  Michael F. Jacobson of the Center for Science ...                    \n",
       "736  A U.S. senator says the lobbying presence in C...                    \n",
       "737  Corrections appearing in print on Saturday, Au...                    \n",
       "738  A discussion of the need for political engagem...                    \n",
       "\n",
       "           genre                                            article  \n",
       "0       Business  Lupii bars were supposed to have a breakout ye...  \n",
       "1       Business  It has been a staple of American breakfast tab...  \n",
       "2       Business  PepsiCo plans to announce a new venture on Wed...  \n",
       "3          Style  PepsiCo announced on Friday that it would no l...  \n",
       "4       Business  PepsiCo said Monday that it would buy back up ...  \n",
       "..           ...                                                ...  \n",
       "734  Corrections  NATIONAL An article on Monday about President ...  \n",
       "735      Opinion  To the Editor:  “Social Media as a Megaphone t...  \n",
       "736      Opinion  To the Editor: Re “Soda Giants Back, Then Lobb...  \n",
       "737  Corrections  BUSINESS DAY An article on Thursday about a ne...  \n",
       "738      Opinion  To the Editor: Re “Progress Seen on Warming, W...  \n",
       "\n",
       "[739 rows x 6 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[['date','title','abstract','author','genre','article']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df[df['article']==''])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "date     677\n",
       "title    812\n",
       "dtype: int64"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[['date','title']].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_stored[df_stored['article']==''])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_excel(r'C:\\Users\\woody\\iCloudDrive\\First-year MSOR\\MS Columbia 2nd data challenge\\NYT_Disney.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
